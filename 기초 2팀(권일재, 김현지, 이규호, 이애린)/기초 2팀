MNIST CNN dropout, ensemble  99per acc acheive
Spyder Editor

This is a temporary script file.
"""

import tensorflow as tf
import random
from tensorflow.examples.tutorials.mnist import input_data
tf.set_random_seed(777)
mnist=input_data.read_data_sets('MNIST_data/', one_hot=True)
learning_rate=0.001
training_epochs=20
batch_size=100


class Module:
    def __init__(self, sess, name):
        self.sess=sess
        self.name=name
        self._buildnet()
    def _buildnet(self):
        with tf.variable_scope(self.name):
            self.training=tf.placeholder(tf.bool)
            self.X=tf.placeholder(tf.float32,[None,784])
            X_=tf.reshape(self.X,[-1,28,28,1])
            self.Y=tf.placeholder(tf.float32,[None,10])
            
            conv1=tf.layers.conv2d(inputs=X_, filters=32, kernel_size=[3,3], padding='SAME', activation=tf.nn.relu)
            pool1=tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2], padding='SAME', strides=2)
            dropout1=tf.layers.dropout(inputs=pool1, rate=0.7, training=self.training)
            
            conv2=tf.layers.conv2d(inputs=dropout1, filters=32, kernel_size=[3,3], padding='SAME', activation=tf.nn.relu)
            pool2=tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2], padding='SAME', strides=2)
            dropout2=tf.layers.dropout(inputs=pool2, rate=0.7, training=self.training)
            
            conv3=tf.layers.conv2d(inputs=dropout2, filters=32, kernel_size=[3,3], padding='SAME',activation =tf.nn.relu)
            pool3=tf.layers.max_pooling2d(inputs=conv3, pool_size=[2,2], padding='SAME', strides=2)
            dropout3=tf.layers.dropout(inputs=pool3, rate=0.7, training=self.training)
            self.logits=dropout3
            
        self.cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))
        self.optimizer=tf.train.AdamOptimizer(learing_rate=learning_rate).minimize(cost)
        
        corrpred=tf.equal(tf.argmax(self.logits, 1),tf.argmax(self.Y, 1))
        self.accuracy=tf.reduce_mean(tf.cast(corrpred, tf.float32))
    def predict(self, x_test, y_test, training=False):
        return self.sess.run(self.logits, feed_dict={self.X:x_test, self.Y:y_test, self.training:training} )
    def get_acc(self, x_test, y_test, training=False):
        return self.sess.run(self.accuracy, feed_dict={self.X:x_test, self.Y:y_test, self.training:training})
    def train(self, x_data, y_data, training=True):
        return self.sess.run(self.optimizer, feed_dict={self.X:x_data, self.Y:y_data, self.training:training})
    
sess= tf.Session
sess.run(tf.global_variables_initializer())

models=[]
num_models=2
for m in range(num_models):
    models.append(Model(sess,'Model'+str(m)))
print('machine learning STarted')

for epoch in range(training_epochs):
    avg_cost_list=np.zeros(len(models))
    total_batch=int(mnist.train.num_examples/batch_size)
    for i in range(totla_batch):
        batch_x, batch_y= mnist.train.next_batch(batch_size)
        
        for idx,m in enumerate(models):
            c, _=m.train(batch_x, batch_y)
            avg_cost_list[idx] += c/total_batch
    print('Epoch:', '%04d'%(epoch+1), 'cost:', avg_cost_list)
    tf.train.Saver(var_list=Model1.accuracy, Model2.accuracy)

print('learing finished')


test_size=len(mnist.test.labels)
pred=np.zeros(test_size*10)
for idx, m in enumerate(models):
    print(idx, 'Accuracy',m.get_acc(mnist.test.images,mnist.test.labels))
    p=m.predict(mnist.test.images)
    pred += p
    
ensemble_correct_prediction=tf.equal(tf.argmax(predict,1),tf.argmax(mnist.test.labels,1))
ensemble_acc=tf.reduce_mean(tf.cast(ensemble_correct_prediction, tf.float32)
print('Ensemble Accuracy',ensemble_acc)
